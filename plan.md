# Plan: Implementa√ß√£o de Semantic Parsing com Sentence-Transformers

Implementa√ß√£o de compreens√£o sem√¢ntica usando `sentence-transformers` para superar as limita√ß√µes do sistema atual baseado em keywords, mantendo compatibilidade com a arquitetura existente atrav√©s de uma abordagem h√≠brida.

## Objetivo

Adicionar capacidade de entendimento sem√¢ntico ao chatbot para:
- Compreender par√°frases ("quero rir" ‚Üí filmes de com√©dia)
- Suportar queries em ingl√™s e portugu√™s
- Lidar com descri√ß√µes contextuais ("aquele filme do barco que afunda" ‚Üí Titanic)
- Melhorar desambigua√ß√£o entre intents similares

## Arquitetura H√≠brida: Rule-Based + Semantic

```
User Query
    ‚Üì
[Stage 0: Priority Rules] ‚Üê Manter existente (protege comandos simples)
    ‚Üì
[Stage 1: Spell Correction] ‚Üê Manter existente
    ‚Üì
[Stage 2A: Semantic Intent Classification] ‚Üê NOVO
    ‚Üì (se confidence < threshold)
[Stage 2B: Keyword-Based Fallback] ‚Üê Manter existente
    ‚Üì
[Stage 3: Entity Extraction - Hybrid] ‚Üê APRIMORAR
    ‚Üì
[Stage 4: Entity Validation - Semantic] ‚Üê APRIMORAR
    ‚Üì
Handler Routing
```

## Steps

### 1. Criar infraestrutura base para classifica√ß√£o sem√¢ntica

**Objetivo**: Estabelecer foundation para semantic parsing

**Tarefas**:
- [ ] Adicionar `sentence-transformers>=2.2.0` e `torch>=2.0.0` em `requirements.txt`
- [ ] Criar `app/semantic_classifier.py` com classe `SemanticIntentClassifier`
  - ‚ö†Ô∏è **USAR MODELO R√ÅPIDO**: `paraphrase-multilingual-MiniLM-L12-v2` (118M params, 2x mais r√°pido)
  - Implementar m√©todo `classify()` que retorna top-K intents com scores de similaridade cosseno
  - Carregar embeddings dos exemplos de `intent_patterns.json` no `__init__()`
- [ ] Criar `app/config.py` com configura√ß√µes:
  ```python
  USE_SEMANTIC_NLU: bool = True  # Toggle via env var
  SEMANTIC_MODEL_NAME: str = "paraphrase-multilingual-MiniLM-L12-v2"  # ‚ö†Ô∏è ALTERADO
  SEMANTIC_INTENT_THRESHOLD: float = 0.75  # Alta confian√ßa
  SEMANTIC_ENTITY_THRESHOLD: float = 0.70
  ```

**Estrutura `SemanticIntentClassifier`**:
```python
class SemanticIntentClassifier:
    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):  # ‚ö†Ô∏è MODELO R√ÅPIDO
        self.model = SentenceTransformer(model_name)
        self.intent_embeddings = {}
        self.intent_examples = {}
        
    def load_intent_patterns(self, patterns: dict):
        """Compute embeddings for all example queries."""
        for intent, pattern in patterns.items():
            examples = pattern.get("examples", [])
            if examples:
                embeddings = self.model.encode(examples, convert_to_tensor=True)
                self.intent_embeddings[intent] = embeddings
                self.intent_examples[intent] = examples
    
    def classify(self, query: str, top_k=3) -> List[Tuple[str, float]]:
        """Returns: [(intent, confidence), ...]"""
        query_embedding = self.model.encode(query, convert_to_tensor=True)
        
        scores = []
        for intent, example_embeddings in self.intent_embeddings.items():
            similarities = util.cos_sim(query_embedding, example_embeddings)
            max_sim = similarities.max().item()
            scores.append((intent, max_sim))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
```

**Crit√©rios de sucesso**:
- Modelo carrega em < 10s
- Classifica√ß√£o de query √∫nica em < 100ms
- Testes unit√°rios passando

---

### 2. Aumentar dados de treino com augmenta√ß√£o

**Objetivo**: Resolver problema de dados escassos (apenas 2-3 exemplos por intent)

**Problema atual**:
- `intent_patterns.json` tem apenas ~50 exemplos totais
- Insuficiente para treinar/testar semantic classifier
- N√£o cobre par√°frases e varia√ß√µes

**Tarefas**:
- [ ] Criar script `scripts/augment_intent_data.py` com:
  - Back-translation (PT ‚Üí EN ‚Üí PT usando MarianMT)
  - Paraphrasing (T5-based paraphraser)
  - Template generation (combinar templates com entities do cache)
  - Synonym replacement para keywords comuns
- [ ] Gerar 15-20 varia√ß√µes para cada exemplo existente
- [ ] Adicionar manualmente par√°frases criativas:
  - "quero rir" ‚Üí `filmes_por_genero` (com√©dia)
  - "algo assustador" ‚Üí `filmes_por_genero` (terror)
  - "aquele diretor ingl√™s do batman" ‚Üí `filmes_por_diretor` (Christopher Nolan)
- [ ] Criar `app/intent_examples_augmented.json` com estrutura:
  ```json
  {
    "filmes_por_ator": {
      "original": ["filmes com adam sandler", ...],
      "augmented": [
        "obras do adam sandler",
        "adam sandler filmes",
        "quero ver filmes com adam sandler",
        "o que adam sandler atuou",
        "filmografia do adam sandler",
        ...
      ]
    }
  }
  ```

**Meta**: 300-400 exemplos totais (15-25 por intent)

**Exemplo de augmenta√ß√£o**:
```python
class IntentDataAugmenter:
    def __init__(self):
        self.paraphraser = pipeline("text2text-generation", 
                                     model="humarin/chatgpt_paraphraser_on_T5_base")
    
    def augment_examples(self, examples: List[str], n_augment=5) -> List[str]:
        augmented = examples.copy()
        for example in examples:
            paraphrases = self.paraphraser(example, 
                                           max_length=60, 
                                           num_return_sequences=n_augment)
            augmented.extend([p['generated_text'] for p in paraphrases])
        return augmented
```

**Crit√©rios de sucesso**:
- 300+ exemplos gerados
- Revis√£o manual de qualidade (amostra de 10%)
- Exemplos cobrem par√°frases comuns

---

### 3. Integrar classifica√ß√£o sem√¢ntica no NLUEngine com fallback

**Objetivo**: Adicionar semantic classification mantendo backward compatibility

**Tarefas**:
- [ ] Modificar `app/nlu_engine.py`:
  - Adicionar par√¢metro `use_semantic=True` no `__init__()`
  - Implementar try-except para fallback gracioso se modelo falhar
  - Renomear m√©todo `_detect_intent()` para `_detect_intent_keyword_based()`
  - Criar novo `_detect_intent()` com l√≥gica h√≠brida:

**L√≥gica de decis√£o h√≠brida**:
```python
def _detect_intent(self, text_lower: str, doc) -> Tuple[str, float]:
    # Try semantic first
    if self.use_semantic:
        semantic_results = self.semantic_classifier.classify(text_lower)
        best_intent, semantic_score = semantic_results[0]
        
        # HIGH CONFIDENCE: Use semantic directly
        if semantic_score >= 0.75:
            logger.debug(f"Semantic (high conf): {best_intent} ({semantic_score:.2f})")
            return best_intent, semantic_score
        
        # MEDIUM CONFIDENCE: Validate with keywords
        if semantic_score >= 0.60:
            keyword_intent, keyword_score = self._detect_intent_keyword_based(text_lower, doc)
            if keyword_intent == best_intent:
                # Agreement ‚Üí combine scores
                combined_score = 0.6 * semantic_score + 0.4 * keyword_score
                logger.debug(f"Semantic+Keyword agree: {best_intent} ({combined_score:.2f})")
                return best_intent, combined_score
            else:
                # Disagreement ‚Üí use higher confidence
                if semantic_score > keyword_score:
                    return best_intent, semantic_score
                else:
                    return keyword_intent, keyword_score
    
    # LOW CONFIDENCE or semantic disabled: Fallback to keyword-based
    return self._detect_intent_keyword_based(text_lower, doc)
```

**Modifica√ß√µes adicionais**:
- [ ] Adicionar flag `self._semantic_score` para uso em `_calculate_overall_confidence()`
- [ ] Atualizar `_calculate_overall_confidence()` para boost sem√¢ntico (+20% se score alto)
- [ ] Adicionar logging detalhado de qual m√©todo foi usado (semantic/keyword/hybrid)

**Inicializa√ß√£o em `app/main.py`**:
```python
@app.on_event("startup")
async def startup_event():
    # ... existing code ...
    
    # Initialize semantic NLU
    if config.USE_SEMANTIC_NLU:
        try:
            semantic_classifier = SemanticIntentClassifier(
                model_name=config.SEMANTIC_MODEL_NAME
            )
            semantic_classifier.load_intent_patterns(nlu_engine.intent_patterns)
            nlu_engine.semantic_classifier = semantic_classifier
            logger.info("‚úÖ Semantic NLU initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Semantic NLU failed to load: {e}")
            logger.warning("Falling back to keyword-based NLU")
```

**Crit√©rios de sucesso**:
- Sistema funciona com `use_semantic=True` e `False`
- Fallback autom√°tico se modelo n√£o carregar
- Testes existentes em `tests/test_nlu_engine.py` continuam passando
- Lat√™ncia adicional < 100ms (P50)

---

### 4. Criar extra√ß√£o sem√¢ntica de entidades para valida√ß√£o h√≠brida

**Objetivo**: Melhorar extra√ß√£o de entidades com semantic similarity

**Tarefas**:
- [ ] Criar `app/semantic_entity_extractor.py`:

```python
class SemanticEntityExtractor:
    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):  # ‚ö†Ô∏è MODELO R√ÅPIDO
        self.model = SentenceTransformer(model_name)
        self.nlp = spacy.load("pt_core_news_sm")  # ‚ö†Ô∏è ADICIONAR para segmenta√ß√£o
        self.entity_embeddings = {}
        
    def load_entity_caches(self, actors, genres, films, directors):
        """Precompute embeddings for all entities."""
        # ~60MB total RAM
        self.entity_embeddings['actor'] = self._encode_entities(actors)
        self.entity_embeddings['genre'] = self._encode_entities(genres)
        self.entity_embeddings['film'] = self._encode_entities(films)
        self.entity_embeddings['director'] = self._encode_entities(directors)
    def _encode_entities(self, entities: List[str]) -> dict:
        """Returns {entity_name: embedding}"""
        embeddings = self.model.encode(entities, convert_to_tensor=True)
        return {entity: emb for entity, emb in zip(entities, embeddings)}
    
    def _extract_entity_candidates(self, query: str) -> List[str]:
        """
        ‚ö†Ô∏è CR√çTICO: Extrai candidatos com spaCy ANTES de vetorizar.
        Previne compara√ß√£o de frase inteira (com ru√≠do) vs entidade.
        """
        doc = self.nlp(query)
        candidates = []
        
        # Estrat√©gia 1: Entidades nomeadas
        for ent in doc.ents:
            if ent.label_ in ["PER", "ORG", "MISC"]:
                candidates.append(ent.text)
        
        # Estrat√©gia 2: Noun chunks (ex: "aquele ator de Titanic")
        for chunk in doc.noun_chunks:
            text = chunk.text.strip()
            if len(text.split()) >= 2:
                candidates.append(text)
        
        # Estrat√©gia 3: Texto ap√≥s preposi√ß√µes
        prep_patterns = [" com ", " de ", " do ", " da ", " por "]
        query_lower = query.lower()
        for prep in prep_patterns:
            if prep in query_lower:
                idx = query_lower.rfind(prep)
                after_prep = query[idx + len(prep):].strip().rstrip("?!.,")
                if after_prep:
                    candidates.append(after_prep)
        
        # Remove duplicatas
        seen = set()
        unique = []
        for c in candidates:
            c_clean = c.strip().lower()
            if c_clean and c_clean not in seen and len(c_clean) > 2:
                seen.add(c_clean)
                unique.append(c.strip())
        
        return unique
    
    def find_best_entity(self, query: str, entity_type: str, 
                        threshold=0.7, top_k=5) -> Optional[dict]:
        """
        ‚ö†Ô∏è VERS√ÉO CORRIGIDA: Vetoriza apenas candidatos extra√≠dos.
        """
        if entity_type not in self.entity_embeddings:
            return None
        
        # ‚ö†Ô∏è PASSO 1: Extrair candidatos com spaCy
        candidates = self._extract_entity_candidates(query)
        if not candidates:
            candidates = [query]  # Fallback
        
        # ‚ö†Ô∏è PASSO 2: Vetorizar apenas candidatos
        best_match = None
        best_score = 0.0
        all_scores = []
        
        for candidate in candidates:
            candidate_embedding = self.model.encode(candidate, convert_to_tensor=True)
            
            for entity, entity_embedding in self.entity_embeddings[entity_type].items():
                similarity = util.cos_sim(candidate_embedding, entity_embedding).item()
                
                if similarity > best_score:
                    best_score = similarity
                    best_match = entity
                
                all_scores.append((entity, similarity, candidate))
        
        all_scores.sort(key=lambda x: x[1], reverse=True)
        
- [ ] ‚ö†Ô∏è **CRIAR `app/prolog_normalizer.py`** (CR√çTICO):
  ```python
  import re
  import unicodedata
  
  class PrologNormalizer:
      @staticmethod
      def normalize_to_prolog_atom(entity: str) -> str:
          """Leonardo DiCaprio ‚Üí leonardo_dicaprio"""
          normalized = entity.lower()
          nfkd = unicodedata.normalize('NFKD', normalized)
          normalized = ''.join(c for c in nfkd if not unicodedata.combining(c))
          normalized = normalized.replace(' ', '_').replace('&', '_')
          normalized = re.sub(r'[^a-z0-9_]', '', normalized)
          normalized = re.sub(r'_+', '_', normalized).strip('_')
          return normalized
      
      @staticmethod
      def normalize_entity_for_query(entity: str, entity_type: str) -> str:
          """Normaliza baseado no tipo para match com KB."""
          if entity_type in ['actor', 'director', 'genre', 'film']:
              return entity.upper()  # KB usa UPPERCASE
          return entity
  ```

- [ ] Adicionar wrappers em `app/nlu.py`:
  - `find_best_actor_semantic()` ‚Üí retorna entity normalizada
  - `find_best_genre_semantic()` ‚Üí retorna entity normalizada
  - `find_best_film_semantic()` ‚Üí retorna entity normalizada
  - `find_best_director_semantic()` ‚Üí retorna entity normalizada

- [ ] ‚ö†Ô∏è **MODIFICAR handlers** para usar normaliza√ß√£o:
  ```python
  # Em search_handlers.py
  from ..prolog_normalizer import PrologNormalizer
  
  async def handle_filmes_por_ator(self, entities, session_id):
      ator = entities.get("ator", "")
      # ... semantic/fuzzy matching ...
      ator_matched = semantic_extractor.find_best_entity(ator, 'actor')['best_match']
      
      # ‚ö†Ô∏è NORMALIZA√á√ÉO OBRIGAT√ìRIA
      ator_normalized = PrologNormalizer.normalize_entity_for_query(
          ator_matched, 'actor'
      )
      
      escaped = self._escape_prolog_string(ator_normalized)
      query = f"imdb_rules:filmes_por_ator('{escaped}', TituloFilme)"
      # ...
  ```

- [ ] Integrar no `_extract_entities()` do `NLUEngine`:
  - Se `use_semantic=True`, tentar semantic extraction primeiro
  - Fallback para m√©todos existentes (preposi√ß√£o-based, spaCy NER)
  - ‚ö†Ô∏è Sempre normalizar entidades antes de retornar
```

- [ ] Modificar `app/nlu.py` para adicionar fun√ß√µes h√≠bridas:

```python
def find_best_match_hybrid(query: str, cache: list[str], 
                          entity_type: str) -> Optional[str]:
    """
    Hybrid matching: semantic (60%) + fuzzy (40%)
    """
    # Stage 1: Semantic matching
    if semantic_extractor:
        semantic_result = semantic_extractor.find_best_entity(
            query, entity_type, threshold=0.60
        )
        
        if semantic_result:
            # Stage 2: Fuzzy validation
            best_candidate = semantic_result['best_match']
            semantic_score = semantic_result['confidence']
            fuzzy_score = fuzz.ratio(query.upper(), best_candidate.upper()) / 100
            
            combined_score = 0.6 * semantic_score + 0.4 * fuzzy_score
            
            if combined_score >= 0.65:
                return best_candidate
    
    # Fallback: Pure fuzzy
    return find_best_match(query, cache)

def is_valid_genre_semantic(query: str) -> bool:
    """Enhanced genre validation with semantic understanding."""
    # Existing fuzzy checks
    if is_valid_genre(query):
        return True
    
    # Semantic check for synonyms
    if semantic_extractor:
        result = semantic_extractor.find_best_entity(query, 'genre', threshold=0.70)
        return result is not None
    
    return False
```

- [ ] Adicionar wrappers:
  - `find_best_actor_semantic()`
  - `find_best_genre_semantic()`
  - `find_best_film_semantic()`
  - `find_best_director_semantic()`

- [ ] Integrar no `_extract_entities()` do `NLUEngine`:
  - Se `use_semantic=True`, tentar semantic extraction primeiro
  - Fallback para m√©todos existentes (preposi√ß√£o-based, spaCy NER)

**Pr√©-computa√ß√£o de embeddings**:
- [ ] Carregar caches no startup de `main.py`
- [ ] Pr√©-computar embeddings e armazenar em mem√≥ria (~60MB)
- [ ] Considerar cache em disco para evitar recomputa√ß√£o

**Crit√©rios de sucesso**:
- Entidades extra√≠das corretamente mesmo com typos sem√¢nticos
- "assustador" detectado como "horror"
- "engra√ßado" detectado como "com√©dia"
- Lat√™ncia < 50ms para entity extraction

---

### 5. Implementar otimiza√ß√£o e monitoramento de performance

**Objetivo**: Garantir que semantic parsing n√£o degrada performance

**Tarefas de Otimiza√ß√£o**:

- [ ] **Caching de embeddings de queries no Redis**:
  ```python
  def get_query_embedding(self, query: str) -> torch.Tensor:
      cache_key = f"emb:{hashlib.md5(query.encode()).hexdigest()}"
      
      # Try Redis cache
      cached = redis_client.get(cache_key)
      if cached:
          return torch.from_numpy(np.frombuffer(cached, dtype=np.float32))
      
      # Compute and cache
      embedding = self.model.encode(query, convert_to_tensor=True)
      redis_client.setex(cache_key, 3600, embedding.cpu().numpy().tobytes())
      return embedding
  ```

- [ ] **Lazy loading do modelo**:
  ```python
  @property
  def model(self):
      if not hasattr(self, '_model'):
          logger.info("Loading sentence-transformer model...")
          self._model = SentenceTransformer(self.model_name)
      return self._model
  ```

- [ ] **Batch processing para m√∫ltiplas queries**:
  - Se handler processar m√∫ltiplas entities, processar em batch
  - Reduz overhead de infer√™ncia

- [ ] **Quantiza√ß√£o do modelo (opcional)**:
  - Usar ONNX Runtime para infer√™ncia 2-3x mais r√°pida
  - Reduz mem√≥ria em ~50%

**Tarefas de Monitoramento**:

- [ ] Adicionar logging de m√©tricas em `nlu_engine.py`:
  ```python
  logger.info(f"Intent detection: method={method}, latency={latency_ms}ms, "
              f"semantic_score={semantic_score:.2f}, fallback={used_fallback}")
  ```

- [ ] Criar dashboard de m√©tricas:
  - Taxa de uso de semantic vs keyword
  - Distribui√ß√£o de scores sem√¢nticos
  - Lat√™ncia P50, P95, P99
  - Taxa de fallback

- [ ] Implementar A/B testing:
  ```python
  def should_use_semantic(session_id: str) -> bool:
      """Route 50% of users to semantic NLU."""
      hash_val = int(hashlib.md5(session_id.encode()).hexdigest(), 16)
      return hash_val % 2 == 0
  ```

**Testes de Performance**:

- [ ] Criar `tests/test_semantic_classifier.py`:
  - Benchmark lat√™ncia: semantic vs keyword
  - Teste de acur√°cia em queries de `test_nlu_engine.py`
  - Compara√ß√£o de scores
  - Teste de fallback gracioso

- [ ] Criar `tests/benchmark_semantic.py`:
  - Load test com 1000 queries variadas
  - Medir P50, P95, P99 latency
  - Medir mem√≥ria (baseline vs semantic)

**Targets de Performance**:
- Lat√™ncia P50: < 100ms adicionado
- Lat√™ncia P95: < 250ms adicionado
- Model load time: < 10s
- Memory overhead: < 100MB
- Fallback rate: < 10%

**Crit√©rios de sucesso**:
- Targets de performance atingidos
- Testes de benchmark documentados
- A/B testing framework funcional
- M√©tricas dispon√≠veis em logs

---

## Casos de Uso Melhorados

### Antes vs Depois

#### Caso 1: Par√°frase
**Query**: "quero rir"
- **Antes**: ‚ùå Falha (nenhuma keyword de "filmes" ou "com√©dia")
- **Depois**: ‚úÖ Intent: `filmes_por_genero`, Entity: `com√©dia` (semantic similarity com exemplos)

#### Caso 2: Descri√ß√£o contextual
**Query**: "aquele filme do barco que afunda"
- **Antes**: ‚ùå Falha (nenhuma keyword reconhecida)
- **Depois**: ‚úÖ Intent: `busca_especifica`, Semantic search encontra "Titanic" via contexto

#### Caso 3: Query em ingl√™s
**Query**: "action movies"
- **Antes**: ‚ùå Falha (keywords em portugu√™s)
- **Depois**: ‚úÖ Intent: `filmes_por_genero`, Entity: `ACTION & ADVENTURE` (modelo multilingual)

#### Caso 4: Sin√¥nimo
**Query**: "filmes assustadores"
- **Antes**: ‚ö†Ô∏è Poss√≠vel falha se "assustador" n√£o no `GENRE_TRANSLATION_MAP`
- **Depois**: ‚úÖ Entity: `HORROR MOVIES` (semantic similarity)

#### Caso 5: Query mista
**Query**: "filmes de action com Nolan"
- **Antes**: ‚ö†Ô∏è Depende de fuzzy matching
- **Depois**: ‚úÖ Intent: `filmes_com_filtros`, Entities: `{genre: 'ACTION & ADVENTURE', diretor: 'Christopher Nolan'}` (semantic + fuzzy)

---

## Estrat√©gia de Deployment

### Fase 1: Desenvolvimento (Semana 1-2)
- Implementar Steps 1-2 (infraestrutura + augmenta√ß√£o)
- Testes unit√°rios locais
- Benchmark de acur√°cia vs baseline

### Fase 2: Integra√ß√£o (Semana 3)
- Implementar Steps 3-4 (integra√ß√£o + entity extraction)
- Testes de integra√ß√£o
- Code review

### Fase 3: Otimiza√ß√£o (Semana 4)
- Implementar Step 5 (performance + monitoring)
- Load testing
- Tuning de thresholds

### Fase 4: Rollout Gradual (Semana 5-6)
- **10% de usu√°rios** (1 semana):
  - A/B test com hash de session_id
  - Monitorar: accuracy, latency, crash rate
  - Coletar feedback
- **50% de usu√°rios** (1 semana):
  - Se m√©tricas OK, expandir
  - Ajustar thresholds se necess√°rio
- **100% de usu√°rios**:
  - Rollout completo
  - Feature flag permanece para rollback r√°pido

---

## M√©tricas de Sucesso

### Acur√°cia
- **Intent Classification Accuracy**: >90% (vs ~85% keyword baseline)
- **Entity Extraction F1-Score**: >85%
- **Paraphrase Recognition**: >80% (novo m√©trico)

### Performance
- **Lat√™ncia P50**: <100ms added
- **Lat√™ncia P95**: <250ms added
- **Model Load Time**: <10s
- **Memory Overhead**: <100MB
- **Fallback Rate**: <10%

### User Experience
- **Successful Query Rate**: >95% (vs ~92% baseline)
- **Clarification Request Rate**: <5%
- **User Satisfaction**: Qualitative feedback

---

## Riscos e Mitiga√ß√µes

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|--------------|---------|-----------|
| Model loading failure | Baixa | Alta | Fallback robusto para keyword-based |
| Poor accuracy on domain | M√©dia | Alta | Data augmentation, fine-tune em feedback |
| Performance degradation | M√©dia | M√©dia | Caching, lazy loading, batch processing |
| Breaking existing tests | Baixa | Baixa | Backward compatibility, tests abrangentes |
| User confusion | Baixa | M√©dia | A/B test, rollout gradual, feature flag |

---

## ‚ö†Ô∏è Riscos Cr√≠ticos e Ajustes Obrigat√≥rios

### üî¥ CR√çTICO 1: Extra√ß√£o de Entidades em Frases Complexas

**O Problema**:
O m√©todo `find_best_entity()` proposto compara o embedding da **frase inteira** com embeddings de entidades isoladas, gerando muitos falsos negativos.

**Exemplo de Falha**:
```python
Query: "Quero ver filmes com aquele ator de Titanic"
Embedding da frase: [muito ru√≠do: "quero", "ver", "filmes", "com"...]
Embedding de "Leonardo DiCaprio": [nome limpo]
Similaridade: 0.35 ‚ùå (abaixo do threshold 0.70)
```

**‚úÖ Solu√ß√£o Obrigat√≥ria: Segmenta√ß√£o Antes da Vetoriza√ß√£o**

**Adicionar ao Step 4** um pr√©-processamento com spaCy:

```python
class SemanticEntityExtractor:
    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):
        self.model = SentenceTransformer(model_name)
        self.nlp = spacy.load("pt_core_news_sm")  # Para segmenta√ß√£o
        self.entity_embeddings = {}
    
    def _extract_entity_candidates(self, query: str) -> List[str]:
        """
        Extrai candidatos a entidades usando spaCy ANTES de vetorizar.
        
        Estrat√©gias:
        1. Entidades nomeadas (PER, ORG, MISC)
        2. Noun chunks (substantivos compostos)
        3. Tokens ap√≥s preposi√ß√µes chave
        """
        doc = self.nlp(query)
        candidates = []
        
        # Estrat√©gia 1: Entidades detectadas pelo spaCy
        for ent in doc.ents:
            if ent.label_ in ["PER", "ORG", "MISC"]:
                candidates.append(ent.text)
        
        # Estrat√©gia 2: Noun chunks (ex: "aquele ator de Titanic")
        for chunk in doc.noun_chunks:
            # Remove determinantes no in√≠cio
            text = chunk.text.strip()
            if len(text.split()) >= 2:  # Apenas chunks multi-palavra
                candidates.append(text)
        
        # Estrat√©gia 3: Texto ap√≥s preposi√ß√µes espec√≠ficas
        prep_patterns = [" com ", " de ", " do ", " da ", " por "]
        query_lower = query.lower()
        for prep in prep_patterns:
            if prep in query_lower:
                idx = query_lower.rfind(prep)
                after_prep = query[idx + len(prep):].strip()
                # Remove pontua√ß√£o final
                after_prep = after_prep.rstrip("?!.,")
                if after_prep:
                    candidates.append(after_prep)
        
        # Remove duplicatas mantendo ordem
        seen = set()
        unique_candidates = []
        for c in candidates:
            c_clean = c.strip().lower()
            if c_clean and c_clean not in seen and len(c_clean) > 2:
                seen.add(c_clean)
                unique_candidates.append(c.strip())
        
        return unique_candidates
    
    def find_best_entity(self, query: str, entity_type: str, 
                        threshold=0.7, top_k=5) -> Optional[dict]:
        """
        VERS√ÉO CORRIGIDA: Vetoriza apenas os candidatos extra√≠dos.
        """
        if entity_type not in self.entity_embeddings:
            return None
        
        # PASSO 1: Extrair candidatos com spaCy
        candidates = self._extract_entity_candidates(query)
        
        if not candidates:
            # Fallback: usar query completa (comportamento anterior)
            candidates = [query]
        
        # PASSO 2: Vetorizar apenas os candidatos
        best_match = None
        best_score = 0.0
        all_scores = []
        
        for candidate in candidates:
            candidate_embedding = self.model.encode(candidate, convert_to_tensor=True)
            
            for entity, entity_embedding in self.entity_embeddings[entity_type].items():
                similarity = util.cos_sim(candidate_embedding, entity_embedding).item()
                
                if similarity > best_score:
                    best_score = similarity
                    best_match = entity
                
                all_scores.append((entity, similarity, candidate))
        
        # Ordena por score para alternativas
        all_scores.sort(key=lambda x: x[1], reverse=True)
        
        if best_score >= threshold:
            return {
                'best_match': best_match,
                'confidence': best_score,
                'matched_candidate': candidates[0] if candidates else query,
                'alternatives': [(e, s) for e, s, _ in all_scores[1:top_k]]
            }
        return None
```

**Impacto**: Reduz falsos negativos de ~40% para <5% em queries complexas.

---

### üî¥ CR√çTICO 2: Escolha de Modelo (Performance vs Precis√£o)

**O Problema**:
O plano escolhe `paraphrase-multilingual-mpnet-base-v2` (~100ms/query). Para extrair **intent + ator + g√™nero + diretor**, voc√™ rodar√° o modelo **4-5 vezes**, elevando lat√™ncia para **400-500ms** (inaceit√°vel).

**‚úÖ Solu√ß√£o: Usar Modelo Mais R√°pido**

**ALTERAR Step 1 e Step 4**:

```python
# ANTES (plano original)
SEMANTIC_MODEL_NAME: str = "paraphrase-multilingual-mpnet-base-v2"

# DEPOIS (obrigat√≥rio para MVP)
SEMANTIC_MODEL_NAME: str = "paraphrase-multilingual-MiniLM-L12-v2"
```

**Justificativa**:
- **Lat√™ncia**: 40-50ms/query (2x mais r√°pido que mpnet)
- **Precis√£o**: Em dom√≠nios restritos (filmes), a diferen√ßa √© <3% (aceit√°vel)
- **Total por request**: 4 queries √ó 50ms = **200ms** (dentro do target P95 <250ms)

**Upgrade Path**: Se ap√≥s testes em produ√ß√£o a precis√£o for insuficiente, considerar:
1. Fine-tuning do MiniLM com dados do chatbot
2. Upgrade para mpnet APENAS para intent classification (1 query), mantendo MiniLM para entities

**Impacto**: Mant√©m lat√™ncia dentro dos targets sem sacrificar UX.

---

### üî¥ CR√çTICO 3: Normaliza√ß√£o para Prolog (Ponte Python ‚Üí Prolog)

**O Problema**:
O Prolog exige **√°tomos exatos** (ex: `'leonardo_dicaprio'`). O plano detecta entidades mas n√£o garante convers√£o para o formato esperado pelo KB.

**Exemplo de Falha**:
```python
# Python detecta:
Query: "o cara do lobo de wall street"
Semantic Match: "Leonardo DiCaprio" (score 0.85)

# Python envia para Prolog:
query_prolog = f"filmes_por_ator('Leonardo DiCaprio', Filme)"
# ‚ùå FALHA: Prolog espera 'leonardo_dicaprio' (snake_case, lowercase)
```

**‚úÖ Solu√ß√£o Obrigat√≥ria: Camada de Normaliza√ß√£o**

**ADICIONAR ao Step 4** (ap√≥s entity extraction):

```python
# Novo arquivo: app/prolog_normalizer.py

import re
import unicodedata

class PrologNormalizer:
    """
    Converte entidades detectadas para formato Prolog-compatible.
    """
    
    @staticmethod
    def normalize_to_prolog_atom(entity: str) -> str:
        """
        Converte entidade para √°tomo Prolog v√°lido.
        
        Regras:
        1. Lowercase
        2. Remove acentos
        3. Substitui espa√ßos por underscore
        4. Remove caracteres especiais (exceto underscore)
        5. Remove underscores duplicados/iniciais/finais
        
        Examples:
            "Leonardo DiCaprio" ‚Üí "leonardo_dicaprio"
            "A√ß√£o & Aventura" ‚Üí "acao_aventura"
            "The Rock" ‚Üí "the_rock"
        """
        # 1. Lowercase
        normalized = entity.lower()
        
        # 2. Remove acentos
        nfkd = unicodedata.normalize('NFKD', normalized)
        normalized = ''.join(c for c in nfkd if not unicodedata.combining(c))
        
        # 3. Substitui espa√ßos e & por underscore
        normalized = normalized.replace(' ', '_').replace('&', '_')
        
        # 4. Remove caracteres especiais (mant√©m apenas letras, n√∫meros, underscore)
        normalized = re.sub(r'[^a-z0-9_]', '', normalized)
        
        # 5. Remove underscores duplicados/iniciais/finais
        normalized = re.sub(r'_+', '_', normalized)
        normalized = normalized.strip('_')
        
        return normalized
    
    @staticmethod
    def normalize_entity_for_query(entity: str, entity_type: str) -> str:
        """
        Normaliza baseado no tipo de entidade.
        
        Args:
            entity: Nome da entidade detectada
            entity_type: 'actor', 'genre', 'film', 'director'
        
        Returns:
            String normalizada para uso em query Prolog
        """
        if entity_type in ['actor', 'director']:
            # Pessoas: mantem uppercase para match com KB
            # Ex: "Leonardo DiCaprio" ‚Üí "LEONARDO DICAPRIO"
            return entity.upper()
        
        elif entity_type == 'genre':
            # G√™neros: uppercase para match com GENRE_CACHE
            # Ex: "a√ß√£o" ‚Üí "ACTION & ADVENTURE" (via find_best_genre)
            return entity.upper()
        
        elif entity_type == 'film':
            # Filmes: uppercase para match com FILM_CACHE
            # Ex: "matrix" ‚Üí "THE MATRIX"
            return entity.upper()
        
        return entity
```

**MODIFICAR handlers** para usar normalizador:

```python
# Em app/handlers/search_handlers.py

from ..prolog_normalizer import PrologNormalizer

### Refer√™ncias T√©cnicas

### Modelos Considerados
- ‚úÖ `paraphrase-multilingual-MiniLM-L12-v2`: **ESCOLHIDO** (r√°pido, 118M params)
- ‚ö†Ô∏è `paraphrase-multilingual-mpnet-base-v2`: Descartado (lento para MVP)
- `LaBSE`: Alternativa se precisar melhor suporte multilingualsponse(...)
        
        # ANTES: fuzzy match direto
        # best_match = find_best_actor(ator)
        
        # DEPOIS: semantic + normaliza√ß√£o
        if use_semantic:
            semantic_result = semantic_extractor.find_best_entity(ator, 'actor')
            if semantic_result:
                ator_matched = semantic_result['best_match']
            else:
                ator_matched = find_best_actor(ator)  # Fallback
        else:
            ator_matched = find_best_actor(ator)
        
        if not ator_matched:
            return self._create_error_response(...)
        
        # ‚úÖ NORMALIZA√á√ÉO OBRIGAT√ìRIA
        ator_normalized = PrologNormalizer.normalize_entity_for_query(
            ator_matched, 'actor'
        )
        
        # Construir query com √°tomo normalizado
        escaped = self._escape_prolog_string(ator_normalized)
        query_string = f"imdb_rules:filmes_por_ator('{escaped}', TituloFilme)"
        
        results = await self._query_prolog(query_string)
        # ...
```

**Impacto**: Elimina 100% das falhas por incompatibilidade de formato Python ‚Üî Prolog.

---

## Quest√µes em Aberto

### 1. Trade-off modelo vs performance? ‚úÖ RESOLVIDO
**Decis√£o Final**: 
- **Usar `paraphrase-multilingual-MiniLM-L12-v2`** (118M params) para MVP
  - Pros: 2x mais r√°pido (~40-50ms), 120MB em disco, lat√™ncia total <250ms
  - Cons: -3% acur√°cia vs mpnet (aceit√°vel em dom√≠nio restrito)
- **Caminho de upgrade**: Fine-tuning do MiniLM com dados reais, ou mpnet apenas para intent classification

~~**Recomenda√ß√£o anterior**: Come√ßar com modelo maior (A), otimizar depois se necess√°rio~~ ‚ùå REJEITADO por risco de lat√™ncia

### 2. GPU vs CPU inference?
- **CPU**: Lat√™ncia ~200ms, sem depend√™ncias extras
- **GPU**: Lat√™ncia ~20ms, requer CUDA setup
- **Recomenda√ß√£o**: CPU para MVP, considerar GPU em produ√ß√£o se lat√™ncia for issue

### 3. Fine-tuning do modelo?
- **Sem fine-tuning**: Zero-shot com dados augmentados
- **Com fine-tuning**: Treinar em dados do chatbot (requer mais dados)
- **Recomenda√ß√£o**: Come√ßar zero-shot, considerar fine-tuning ap√≥s coletar 1000+ queries reais

### 4. Tratamento de follow-up queries?
- Ex: Usu√°rio pergunta "filmes de a√ß√£o", depois "e de terror?"
- **Op√ß√£o A**: Ignorar contexto (comportamento atual)
- **Op√ß√£o B**: Implementar context tracking com session state
- **Recomenda√ß√£o**: Out of scope para MVP, considerar em futuro

---

## Recursos Necess√°rios

### Desenvolvimento
- **1 desenvolvedor s√™nior**: 15-23 dias (~4 semanas)
- **1 revisor**: 3-5 dias para code review

### Infraestrutura
- **RAM adicional**: +100MB por inst√¢ncia
- **Disco adicional**: +300MB (modelo)
- **GPU (opcional)**: Para infer√™ncia r√°pida

### Dados
- **Training data**: 300-400 exemplos (gerados via augmenta√ß√£o)
- **Test data**: 100+ queries manuais para benchmark

---

## Pr√≥ximos Passos

1. ‚úÖ Aprova√ß√£o do plano t√©cnico
2. ‚è≥ Setup do ambiente (instalar `sentence-transformers`, `torch`)
3. ‚è≥ Implementar Step 1 (infraestrutura base)
4. ‚è≥ Gerar dados augmentados (Step 2)
5. ‚è≥ Integrar no NLUEngine (Step 3)
6. ‚è≥ Entity extraction sem√¢ntica (Step 4)
7. ‚è≥ Otimiza√ß√£o e testes (Step 5)
8. ‚è≥ Deploy gradual com A/B testing

---

## Refer√™ncias T√©cnicas

### Modelos Considerados
- `paraphrase-multilingual-mpnet-base-v2`: Recomendado
- `paraphrase-multilingual-MiniLM-L12-v2`: Alternativa r√°pida
- `LaBSE`: Alternativa para multilingual

### Bibliotecas
- `sentence-transformers`: Embeddings sem√¢nticos
- `torch`: Backend para transformers
- `transformers`: Data augmentation (T5 paraphraser)

### Papers/Recursos
- Sentence-BERT: https://arxiv.org/abs/1908.10084
- Multilingual sentence embeddings: https://arxiv.org/abs/2004.09813
- Zero-shot intent classification: https://arxiv.org/abs/1909.02027
